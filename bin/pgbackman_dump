#!/usr/bin/env python
#
# Copyright (c) 2013 Rafael Martinez Guerrero (PostgreSQL-es)
# rafael@postgresql.org.es / http://www.postgresql.org.es/
#
# This file is part of PgBck
# https://github.com/rafaelma/pgbck
#
# PgBck is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# PgBck is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with PgBck.  If not, see <http://www.gnu.org/licenses/>.

import subprocess
import tempfile
import datetime
import sys
import os
import time
import signal
import argparse

sys.path.append('/home/rafael/Devel/GIT/pgbackman')

from pgbackman.logs import *
from pgbackman.database import * 
from pgbackman.config import *

# ############################################
# Function 
# ############################################
    
def pg_dumpall(global_parameters,db):
    '''pg_dumpall function'''
    
    pg_dumpall_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dumpall' + \
        ' -h ' + pgsql_node + \
        ' -p ' + pgsql_node_port + \
        ' -U ' + pgsql_node_admin_user + \
        ' --file=' + global_parameters['cluster_dump_file'] + \
        ' ' + extra_parameters

    try:
        with open(global_parameters['cluster_log_file'],'w') as cluster_log_file:
            
            cluster_log_file.write('------------------------------------\n')
            cluster_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            cluster_log_file.write('Command: ' + pg_dumpall_command + '\n')
            cluster_log_file.write('------------------------------------\n\n')

            cluster_log_file.flush()

            proc = subprocess.Popen([pg_dumpall_command],stdout=cluster_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()
                
            if proc.returncode == 0:
                logs.logger.info('Cluster dump file created - %s',global_parameters['cluster_dump_file'])
                cluster_log_file.write('[OK] Cluster dump file created - ' + global_parameters['cluster_dump_file'] + '\n')
                
                global_parameters['execution_status'] = 'SUCCEEDED'
            else:
                logs.logger.critical('Cluster dump file could not be created. Return code = %s. Check log file: %s',proc.returncode,global_parameters['cluster_log_file'])
                cluster_log_file.write('[ERROR] Cluster dump file could not be created. Return code = ' + str(proc.returncode) + '. Check log file: ' + global_parameters['cluster_log_file'] + '\n')
                
                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)
                sys.exit(1)
                
    except OSError as e:
        logs.logger.critical('Could not generate the final cluster dump file %s - %s',global_parameters['cluster_dump_file'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        register_backup_job_catalog(global_parameters,db)  
        sys.exit(1)
        

# ############################################
# Function 
# ############################################
    
def pg_dump(global_parameters,db):
    '''pg_dump function'''

    if backup_code == 'FULL':
            
        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + pgsql_node + \
            ' -p ' + pgsql_node_port + \
            ' -U ' + pgsql_node_admin_user + \
            ' --file=' + global_parameters['database_dump_file'] + \
            ' --format=c' + \
            ' --blobs' + \
            ' --verbose' + \
            ' ' + extra_parameters + \
            ' ' + dbname         
           
    elif backup_code == 'SCHEMA':
        
        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + pgsql_node + \
            ' -p ' + pgsql_node_port + \
            ' -U ' + pgsql_node_admin_user + \
            ' --schema-only' + \
            ' --file=' + global_parameters['database_dump_file'] + \
            ' --format=c' + \
            ' --verbose' + \
            ' ' + extra_parameters + \
            ' ' + dbname
                        
    elif backup_code == 'DATA':

        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + pgsql_node + \
            ' -p ' + pgsql_node_port + \
            ' -U ' + pgsql_node_admin_user + \
            ' --data-only' + \
            ' --file=' + global_parameters['database_dump_file'] + \
            ' --format=c' + \
            ' --blobs' + \
            ' --verbose' + \
            ' ' + extra_parameters + \
            ' ' + dbname
        
    try:
        with open(global_parameters['database_log_file'],'w') as database_log_file:
            
            database_log_file.write('------------------------------------\n')
            database_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            database_log_file.write('Command: ' + pg_dump_command + '\n')
            database_log_file.write('------------------------------------\n\n')
            
            database_log_file.flush()

            proc = subprocess.Popen([pg_dump_command],stdout=database_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()
                
            if proc.returncode != 0:
                logs.logger.critical('Database dump file could not be created. Return code = %s. Check log file: %s',proc.returncode,global_parameters['database_log_file'])
                
                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)  
                sys.exit(1)
                
            logs.logger.info('Database dump file created - %s',global_parameters['database_dump_file'])
            global_parameters['execution_status'] = 'SUCCEEDED'

    except OSError as e:
        logs.logger.critical('Could not generate the final database dump file %s - %s',global_parameters['database_dump_file'],e)

        global_parameters['execution_status'] = 'ERROR'
        register_backup_job_catalog(global_parameters,db)  
        sys.exit(1)
        

# ############################################
# Function 
# ############################################
    
def pg_dump_users(global_parameters,db):
    '''pg_dump_users function'''
    
    roles = []

    #
    # We need a list with all the roles that own an object in the database we are
    # taking a backup for. We do this in this way:
    #
    # - We generate a temp file with a schema dump of the database
    # - We extract all the roles used in sql statements that use OWNER TO
    # - We extract all roles used in sql statements GRANT .... TO
    # - We generate a temp file with a pg_dumpall -r dump of the cluster running our database
    # - We extract all the lines from the second temp file that have information about some
    #   of the roles extracted from the first temp file
    #
    # We do all this process via temp files insteed of piping the outputs from process to process
    # to avoid problems with huge sql dumps
    #

    # 
    # Extracting roles from the schema dump of the database
    #

    try:
        pg_dump_schema_temp_file = tempfile.NamedTemporaryFile(delete=True,dir=global_parameters['tmp_dir'])
        logs.logger.debug('pg_dump schema temp file created %s',pg_dump_schema_temp_file.name)
    
        try:
            proc = subprocess.Popen([global_parameters['backup_server_pgsql_bin_dir'] + "/pg_dump", \
                                         "-h",pgsql_node, \
                                         "-p",pgsql_node_port, \
                                         "-U",pgsql_node_admin_user, \
                                         "-s",dbname], \
                                        stdout=pg_dump_schema_temp_file, \
                                        stderr=subprocess.STDOUT)
            proc.wait()

            if proc.returncode != 0:
                logs.logger.critical('The command used to generate the tmp schema dump of the database %s has a return value != 0',dbname)

                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)  
                sys.exit(1)
                
            with open(pg_dump_schema_temp_file.name, 'r') as sqldump:
                for line in sqldump:
                     if 'OWNER TO' in line:
                         role = line.split(' OWNER TO ')[1].replace(';\n','')
                         roles.append(role)
                                             
                     if 'GRANT' in line:
                         role = line.split(' TO ')[1].replace(';\n','')
                         
                         if role != 'PUBLIC':
                             roles.append(role)
                             
                unique_role_list = set(roles)
                
            logs.logger.debug('The list of roles we need to restore the database %s has been generated - %s',dbname,unique_role_list)
                        
        except OSError as e:
            logs.logger.critical('Could not generate the tmp schema dump of the database %s - %s',dbname,e)
            
            global_parameters['execution_status'] = 'ERROR'
            register_backup_job_catalog(global_parameters,db)  
            sys.exit(1)
        
    except OSError as e:
        logs.logger.critical('pg_dump schema temp file could not be created in directory %s - %s',global_parameters['tmp_dir'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        register_backup_job_catalog(global_parameters,db)  
        sys.exit(1)
  
    #
    # Extracting sql statements for our roles from the pg_dumpall -r dump of the cluster
    # 
    
    try:
        pg_dumpall_roles_temp_file = tempfile.NamedTemporaryFile(delete=True,dir=global_parameters['tmp_dir'])
        logs.logger.debug('pg_dumpall roles temp file created %s',pg_dumpall_roles_temp_file.name)
    
        try:
            proc = subprocess.Popen([global_parameters['backup_server_pgsql_bin_dir'] + "/pg_dumpall", \
                                         "-r", \
                                         "-h",pgsql_node, \
                                         "-p",pgsql_node_port, \
                                         "-U",pgsql_node_admin_user], \
                                        stdout=pg_dumpall_roles_temp_file, \
                                        stderr=subprocess.STDOUT)
            proc.wait()

            if proc.returncode != 0:
                logs.logger.critical('The command used to generate the role dump for the PgSQL node running the database %s has a return value != 0',dbname)
        
                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)  
                sys.exit(1)

            try:
                with open(global_parameters['roles_dump_file'],'w') as roles_dump_file:

                    roles_dump_file.write('-- \n-- PgBackMan \n-- \n-- Roles needed by the database: \n-- ' + dbname + '@' + pgsql_node + ' \n--\n\n')
                    roles_dump_file.write('BEGIN;\n\n')

                    with open(pg_dumpall_roles_temp_file.name, 'r') as sqldump_roles:
                        for line in sqldump_roles:
                            for role in unique_role_list:
                                
                                #
                                # CREATE ROLE statements
                                #
                                if ' ROLE ' + role + ';' in line:
                                    roles_dump_file.write(line)
                                    
                                #
                                # ALTER ROLE statements
                                #
                                if ' ROLE ' + role + ' ' in line:
                                    roles_dump_file.write(line)
                                        
                        logs.logger.debug('The list of role statements we need from pg_dumpall has been generated')

                    roles_dump_file.write('\nCOMMIT;\n\n')
                    roles_dump_file.write('-- \n-- PgBackMan roles dump completed\n--\n\n')
                    
                logs.logger.info('Final roles dump file created - %s',global_parameters['roles_dump_file'])
                global_parameters['execution_status'] = 'SUCCEEDED'

            except IOError as e:
                logs.logger.critical('Could not generate the final roles dump file %s - %s',global_parameters['roles_dump_file'],e)
                
                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)  
                sys.exit(1)

        except OSError as e:
            logs.logger.critical('Could not generate the tmp role dump of the cluster running the database %s - %s',dbname,e)
            
            global_parameters['execution_status'] = 'ERROR'
            register_backup_job_catalog(global_parameters,db)  
            sys.exit(1)
        
    except OSError as e:
        logs.logger.critical('pg_dumpall role temp file could not be created in directory %s - %s',global_parameters['tmp_dir'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        register_backup_job_catalog(global_parameters,db)  
        sys.exit(1)



# ############################################
# Function 
# ############################################
    
def pg_dump_database_config(global_parameters,db):
    '''pg_dump_database_config function'''

    #
    # Extracting sql statements for our database from the pg_dumpall -s dump of the cluster
    # 
    
    try:
        pg_dumpall_dbconfig_temp_file = tempfile.NamedTemporaryFile(delete=True,dir=global_parameters['tmp_dir'])
        logs.logger.debug('pg_dumpall dbconfig temp file created %s',pg_dumpall_dbconfig_temp_file.name)
    
        try:
            proc = subprocess.Popen([global_parameters['backup_server_pgsql_bin_dir'] + "/pg_dumpall", \
                                         "-s", \
                                         "-h",pgsql_node, \
                                         "-p",pgsql_node_port, \
                                         "-U",pgsql_node_admin_user], \
                                        stdout=pg_dumpall_dbconfig_temp_file, \
                                        stderr=subprocess.STDOUT)
            proc.wait()

            if proc.returncode != 0:
                logs.logger.critical('The command used to generate the dbconfig dump for the PgSQL node running the database %s has a return value != 0',dbname)
                
                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)  
                sys.exit(1)

            try:
                with open(global_parameters['dbconfig_dump_file'],'w') as dbconfig_dump_file:

                    dbconfig_dump_file.write('-- \n-- PgBackMan \n-- \n-- Database attributes needed by the database: \n-- ' + dbname + '@' + pgsql_node + ' \n--\n\n')
                    dbconfig_dump_file.write('BEGIN;\n\n')

                    with open(pg_dumpall_dbconfig_temp_file.name, 'r') as sqldump_dbconfig:
                        for line in sqldump_dbconfig:
                                                         
                            #
                            # ALTER DATABASE statements
                            #
                            if 'ALTER DATABASE ' + dbname + ' SET ' in line:
                                dbconfig_dump_file.write(line)
                                                                  
                    logs.logger.debug('The list of dbconfig statements we need from pg_dumpall has been generated')

                    dbconfig_dump_file.write('\nCOMMIT;\n\n')
                    dbconfig_dump_file.write('-- \n-- PgBackMan dbconfig dump completed\n--\n\n')
                    
                logs.logger.info('Final dbconfig dump file created - %s',global_parameters['dbconfig_dump_file'])
                global_parameters['execution_status'] = 'SUCCEEDED'
                
            except IOError as e:
                logs.logger.critical('Could not generate the final dbconfig dump file %s - %s',global_parameters['dbconfig_dump_file'],e)

                global_parameters['execution_status'] = 'ERROR'
                register_backup_job_catalog(global_parameters,db)  
                sys.exit(1)

        except OSError as e:
            logs.logger.critical('Could not generate the tmp dbconfig dump of the cluster running the database %s - %s',dbname,e)
            
            global_parameters['execution_status'] = 'ERROR'
            register_backup_job_catalog(global_parameters,db)  
            sys.exit(1)
        
    except OSError as e:
        logs.logger.critical('pg_dumpall dbconfig temp file could not be created in directory %s - %s',global_parameters['tmp_dir'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        register_backup_job_catalog(global_parameters,db)  
        sys.exit(1)


# ############################################
# Function handler
# ############################################
    
def get_pgsql_node_dsn(global_parameters,db):
    '''Get the DSN values for a pgsql_node'''

    dsn_value = db.get_pgsql_node_dsn(global_parameters['pgsql_node_id'])

    if dsn_value != None:
        logs.logger.debug('DSN value for PgSQL node %s is %s',pgsql_node,dsn_value)
        return dsn_value
    else:
        logs.logger.critical('PgSQL node: %s is not registered in PgBackMan',pgsql_node)
        sys.exit(1)


# ############################################
# Function handler
# ############################################
    
def get_pgsql_node_release(db):
    '''Get the release pgsql_node is running'''

    pgsql_node_version = str(db.get_server_version())[0:4]
        
    if pgsql_node_version == '9030':
        pgsql_node_release = '9.3'
    elif pgsql_node_version == '9020':
        pgsql_node_release = '9.2'
    elif pgsql_node_version == '9010':
        pgsql_node_release = '9.1'
    elif pgsql_node_version == '9000':
        pgsql_node_release = '9.0'
    elif pgsql_node_version == '8040':
        pgsql_node_release = '8.4'
    else:
        pgsql_node_release = None

    if  pgsql_node_release != None:
        logs.logger.debug('PgSQL node %s is running postgreSQL %s',pgsql_node,pgsql_node_release)
        return pgsql_node_release
    else:
        logs.logger.critical('Could not get the postgreSQL release for this PgSQL node: %s',pgsql_node)
        sys.exit(1)
    
    
# ############################################
# Function handler
# ############################################
    
def get_backup_server_pgsql_bin_dir(global_parameters,db):
    '''Get the directory with postgreSQL binaries to use'''

    pgsql_bin_dir = db.get_backup_server_parameter(global_parameters['backup_server_id'],'pgsql_bin_' + global_parameters['pgsql_node_release'])
    
    if pgsql_bin_dir != None:
        logs.logger.debug('pgsql bin directory to use: %s',pgsql_bin_dir)
        return pgsql_bin_dir
    else:
        logs.logger.critical('Could not get the pgsql bin directory for this PgSQL node: %s',pgsql_node)
        sys.exit(1)


# ############################################
# Function handler
# ############################################
    
def get_filename_id(global_parameters,dump_type,file_type):
    '''Generate the filename used for the backup and log files of a backup job'''
    
    timestamp = datetime.datetime.now().strftime('%Y%m%dT%H%M%S')

    if dump_type == 'CLUSTER':
        filename_id = global_parameters['pgsql_node_backup_dir'] + '/' + file_type + '/' + dump_type + '-' + pgsql_node + '-v' + global_parameters['pgsql_node_release'] + '-j' + def_id + '-c' + backup_code + '-' + timestamp
    else:
        filename_id = global_parameters['pgsql_node_backup_dir'] + '/' + file_type + '/' + dbname + '-' + pgsql_node + '-v' + global_parameters['pgsql_node_release'] + '-j' + def_id + '-c' + backup_code + '-fcustom-' + timestamp + '-' + dump_type 


    return filename_id


# ############################################
# Function handler
# ############################################
    
def check_pgsql_node_backup_directories(global_parameters):
    '''
    Check if the directories needed by pgbackman for a PgSQL node are in place
    If they do not exist, we try to create them.
    '''
    if  global_parameters['pgsql_node_backup_dir'] != None:
        logs.logger.debug('PgSQL node backup partition to use: %s',global_parameters['pgsql_node_backup_dir'])
        
        if os.path.exists(global_parameters['pgsql_node_backup_dir'] + '/dump'):
            logs.logger.debug('Dump directory %s exists',global_parameters['pgsql_node_backup_dir'] + '/dump')
        else:
            logs.logger.error('Dump directory %s does not exist',global_parameters['pgsql_node_backup_dir'] + '/dump')
            
            try:
                os.makedirs(global_parameters['pgsql_node_backup_dir'] + '/dump',0700)
                logs.logger.info('Dump directory %s created',global_parameters['pgsql_node_backup_dir'] + '/dump')
            except OSError as e:
                logs.logger.critical('OS error when creating the dump directory: %s',e)
                sys.exit(1)
                
        if os.path.exists(global_parameters['pgsql_node_backup_dir'] + '/log'):
            logs.logger.debug('Log directory %s exists',global_parameters['pgsql_node_backup_dir'] + '/log')
        else:
            logs.logger.error('Log directory %s does not exist',global_parameters['pgsql_node_backup_dir'] + '/log')
            
            try:
                os.makedirs(global_parameters['pgsql_node_backup_dir'] + '/log',0700)
                logs.logger.info('Log directory %s created',global_parameters['pgsql_node_backup_dir'] + '/log')
            except OSError as e:
                logs.logger.critical('OS error when creating the log directory: %s',e)
                sys.exit(1)       

    else:
        logs.logger.critical('Could not get the PgSQL node backup partition for: %s',pgsql_node)
        sys.exit(1)


# ############################################
# Function handler
# ############################################
  
def register_backup_job_catalog(global_parameters,db):
    '''Update the catalog information in the database'''

    global_parameters['backup_stop'] = datetime.datetime.now()
    duration = global_parameters['backup_stop'] - global_parameters['backup_start']

    #
    # pg_dump_file information
    #
    
    if os.path.exists(global_parameters['cluster_dump_file']):
        pg_dump_file = global_parameters['cluster_dump_file']
        pg_dump_log_file = global_parameters['cluster_log_file']

        try:
            pg_dump_file_size = os.path.getsize(global_parameters['cluster_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump_file: %s - %s',global_parameters['cluster_dump_file'],e)

    elif os.path.exists(global_parameters['database_dump_file']):
        pg_dump_file = global_parameters['database_dump_file']
        pg_dump_log_file = global_parameters['database_log_file']

        try:
            pg_dump_file_size = os.path.getsize(global_parameters['database_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump_file: %s - %s',global_parameters['database_dump_file'],e)

    else:
        pg_dump_file = 'None'
        pg_dump_file_size = 0
    
        if os.path.exists(global_parameters['cluster_log_file']):
            pg_dump_log_file = global_parameters['cluster_log_file']
        elif os.path.exists(global_parameters['database_log_file']):
            pg_dump_log_file = global_parameters['database_log_file']
        else:
            pg_dump_log_file = 'None'

    #
    # pg_dump_roles_file information
    #

    if os.path.exists(global_parameters['roles_dump_file']):
        pg_dump_roles_file = global_parameters['roles_dump_file']
        pg_dump_roles_log_file = global_parameters['roles_log_file']

        try:
            pg_dump_roles_file_size = os.path.getsize(global_parameters['roles_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump roles file: %s - %s',global_parameters['roles_dump_file'],e)
    else:
        pg_dump_roles_file = 'None'
        pg_dump_roles_file_size = 0
        
        if os.path.exists(global_parameters['roles_log_file']):
            pg_dump_roles_log_file = global_parameters['roles_log_file']
        else:
            pg_dump_roles_log_file = 'None'

    #
    # pg_dump_dbconfig_file information
    #

    if os.path.exists(global_parameters['dbconfig_dump_file']):
        pg_dump_dbconfig_file = global_parameters['dbconfig_dump_file']
        pg_dump_dbconfig_log_file = global_parameters['dbconfig_log_file']

        try:
            pg_dump_dbconfig_file_size = os.path.getsize(global_parameters['dbconfig_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump dbconfig file: %s - %s',global_parameters['dbconfig_dump_file'],e)
    else:
        pg_dump_dbconfig_file = 'None'
        pg_dump_dbconfig_file_size = 0
        
        if os.path.exists(global_parameters['dbconfig_log_file']):
            pg_dump_dbconfig_log_file = global_parameters['dbconfig_log_file']
        else:
            pg_dump_dbconfig_log_file = 'None'

    #
    # Updating database
    #

    return_code = db.register_backup_job_catalog(def_id,
                                                 global_parameters['backup_server_id'],  
                                                 global_parameters['pgsql_node_id'],
                                                 dbname,
                                                 global_parameters['backup_start'],
                                                 global_parameters['backup_stop'],
                                                 duration,
                                                 pg_dump_file,
                                                 pg_dump_file_size,
                                                 pg_dump_log_file,
                                                 pg_dump_roles_file,
                                                 pg_dump_roles_file_size,
                                                 pg_dump_roles_log_file,
                                                 pg_dump_dbconfig_file,
                                                 pg_dump_dbconfig_file_size,
                                                 pg_dump_dbconfig_log_file,
                                                 global_parameters['global_log_file'],
                                                 global_parameters['execution_status']
                                                 )
    
    if return_code == True:
        logs.logger.info('Backup job catalog for DefID: %s updated in the database',def_id)
    elif return_code == False:
        logs.logger.info('Problems updating the backup job catalog for DefID: %s in the database',def_id)

        try:
            pending_log_file = global_parameters['pgsql_node_backup_dir']+ '/log/backup_job_catalog_updates_pending.log'

            with open(pending_log_file,'a+') as catalog_pending:
                catalog_pending.write(str(def_id) + ',' +
                                      str(global_parameters['backup_server_id']) + ',' +   
                                      str(global_parameters['pgsql_node_id']) + ',' +   
                                      dbname + ',' +   
                                      str(global_parameters['backup_start']) + ',' +   
                                      str(global_parameters['backup_stop']) + ',' +   
                                      str(duration) + ',' +   
                                      pg_dump_file + ',' +   
                                      str(pg_dump_file_size) + ',' +   
                                      pg_dump_log_file + ',' +   
                                      pg_dump_roles_file + ',' +   
                                      str(pg_dump_roles_file_size) + ',' +   
                                      pg_dump_roles_log_file + ',' +   
                                      pg_dump_dbconfig_file + ',' +   
                                      str(pg_dump_dbconfig_file_size) + ',' +   
                                      pg_dump_dbconfig_log_file + ',' +   
                                      global_parameters['global_log_file'] + ',' + 
                                      global_parameters['execution_status'] +
                                      '\n')
                
                logs.logger.info('Catalog pending log file updated: %s',pending_log_file)
        
        except IOError as e:
            logs.logger.error('Could not generate the catalog pending log file: %s - %s',pending_log_file,e)


# ############################################
# Function handler
# ############################################
    
def signal_handler(signum, frame):
    logs.logger.info('**** pgbackman_dump stopped. ****')
    sys.exit(0)


# ############################################
# Function 
# ############################################
    
def main():
    '''Main function'''
    
    global_parameters = {}

    conf = configuration()
    pgbackman_dsn = conf.dsn

    global_parameters['tmp_dir'] = conf.tmp_dir
    global_parameters['global_log_file'] = conf.log_file

    db = pgbackman_db(pgbackman_dsn,logs,'pgbackman_dump')

    global_parameters['pgsql_node_id'] = db.get_pgsql_node_id(pgsql_node)
    
    pgsql_node_dsn = get_pgsql_node_dsn(global_parameters,db)
    db_pgnode = pgbackman_db(pgsql_node_dsn,logs,'pgbackman_dump')

    # 
    # The backup server FQDN to be used can be defined in the pgbackman configuration file.
    # If the configuration parameter 'backup_server' is not defined, the return value of  
    # socket.getfqdn() will be used.
    #
    # The FQDN of the backup server will be used to find out the internal pgbackman ID of the backup 
    # server
    #

    if conf.backup_server != '':
        backup_server_id = db.get_backup_server_id(conf.backup_server)
        
        if backup_server_id == False:
            logs.logger.critical('Backup server %s does not exist in pgbackman. Stopping pgbackman2cron.',conf.backup_server)
            logs.logger.info('**** pgbackman_dump stopped. ****')
            sys.exit()     
        else:
            logs.logger.debug('Backup server: %s up and running',conf.backup_server)
    else:
        backup_server_id = db.get_backup_server_id(socket.getfqdn())

        if backup_server_id == False:
            logs.logger.critical('Backup server %s does not exist in pgbackman. Stopping pgbackman2cron.',socket.getfqdn())
            logs.logger.info('**** pgbackman_dump stopped. ****')
            sys.exit()     
        else:
            logs.logger.debug('Backup server: %s up and running',socket.getfqdn())

    global_parameters['backup_server_id'] = backup_server_id
 
    global_parameters['pgsql_node_backup_dir'] = db.get_pgsql_node_parameter(global_parameters['pgsql_node_id'],'pgnode_backup_partition')
    global_parameters['pgsql_node_release'] = get_pgsql_node_release(db_pgnode)
    global_parameters['backup_server_pgsql_bin_dir'] = get_backup_server_pgsql_bin_dir(global_parameters,db)
    
    global_parameters['cluster_dump_file'] = get_filename_id(global_parameters,'CLUSTER','dump') + '.sql'
    global_parameters['cluster_log_file'] = get_filename_id(global_parameters,'CLUSTER','log') + '.log'

    global_parameters['database_dump_file'] = get_filename_id(global_parameters,'DATABASE','dump') + '.sql'
    global_parameters['database_log_file'] = get_filename_id(global_parameters,'DATABASE','log') + '.log'

    global_parameters['roles_dump_file'] = get_filename_id(global_parameters,'USERS','dump') + '.sql'
    global_parameters['roles_log_file'] = get_filename_id(global_parameters,'USERS','log') + '.log'
 
    global_parameters['dbconfig_dump_file'] = get_filename_id(global_parameters,'DBCONFIG','dump') + '.sql'
    global_parameters['dbconfig_log_file'] = get_filename_id(global_parameters,'DBCONFIG','log') + '.log'
       
    global_parameters['backup_start'] = datetime.datetime.now()

    check_pgsql_node_backup_directories(global_parameters)

    if backup_code == 'CLUSTER':
        pg_dumpall(global_parameters,db)
    else:
        pg_dump(global_parameters,db)
        pg_dump_users(global_parameters,db)
        pg_dump_database_config(global_parameters,db)
    
    register_backup_job_catalog(global_parameters,db)


# ############################################
# 
# ############################################

if __name__ == '__main__':

    logs = logs("pgbackman_dump")

    signal.signal(signal.SIGINT,signal_handler)
    signal.signal(signal.SIGTERM,signal_handler)

    parser = argparse.ArgumentParser(prog=sys.argv[0])
    parser.add_argument('-H', metavar='PGSQL-NODE', required=True, help='PgSQL node FQDN', dest='pgsql_node')
    parser.add_argument('-p', metavar='PGSQL-NODE-PORT', required=True, help='PgSQL node port', dest='pgsql_node_port')
    parser.add_argument('-U', metavar='PGSQL-NODE_ADMIN-USER', required=True, help='PgSQL node admin user', dest='pgsql_node_admin_user')
    parser.add_argument('-j', metavar='JOBID', required=True, help='Backup job ID', dest='def_id')
    parser.add_argument('-d', metavar='DBNAME', required=False, help='Database name', dest='dbname')
    parser.add_argument('-c', metavar='[FULL|SCHEMA|DATA|CLUSTER]', choices=['FULL', 'SCHEMA', 'DATA', 'CLUSTER'], required=True, help='Backup code', dest='backup_code')
    parser.add_argument('-e', metavar='[FALSE|TRUE]', default=False, required=True, help='Activate encryption', dest='encryption')
    parser.add_argument('-P', metavar='EXTRA-PARAMETERS', required=False, help='extra pg_dump parameters', dest='extra_parameters')

    args = parser.parse_args()    
    print args

    logs.logger.info('**** pgbackman_dump started. ****')
    
    if args.pgsql_node:
        pgsql_node = args.pgsql_node
    else:
        logs.logger.error('PgSQL node parameter not defined')
        sys.exit(1)

    if args.pgsql_node_port:
        pgsql_node_port = args.pgsql_node_port
    else:
        logs.logger.error('PgSQL node port parameter not defined')
        sys.exit(1)
    
    if args.pgsql_node_admin_user:
        pgsql_node_admin_user = args.pgsql_node_admin_user
    else:
        logs.logger.error('PgSQL node admin user parameter not defined')
        sys.exit(1)
    
    if args.def_id:
        def_id = args.def_id
    else:
        logs.logger.error('Backup jobID parameter not defined')
        sys.exit(1)

    if args.dbname:
        dbname = args.dbname
    else:
        dbname = ''

    if args.encryption:
        encryption = args.encryption
    else:
        logs.logger.error('Encryption parameter not defined')
        sys.exit(1)
        
    if args.backup_code:
        backup_code = args.backup_code
    else:
        logs.logger.error('Backup code parameter not defined')
        sys.exit(1)

    if args.extra_parameters:
        extra_parameters = args.extra_parameters
    else:
        extra_parameters = ''

    main()
